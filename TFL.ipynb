{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import joblib\n","from tqdm import tqdm\n","from src.rowwise_metrics import rowwise_cosine, rowwise_euclid\n","from src.metrics import rse\n","\n","# dataset variables\n","from src.visualization import IndexType\n","INDEX = IndexType.HORIZONTAL_SNAKE\n","\n","TRAIN_DIM = (560, 560)\n","VAL_DIM = (266, 500)\n","TEST_DIM = (500, 500)\n","\n","s1_wavelengths = np.load(open('datasets/s1_wavelengths.npy', 'rb'))\n","s2_wavelengths = np.load(open('datasets/s2_wavelengths.npy', 'rb'))"]},{"cell_type":"markdown","metadata":{},"source":["# data and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s1_train = np.load(open(f'datasets/s1_train.npy', 'rb'))\n","s2_train = np.load(open(f'datasets/s2_train.npy', 'rb'))\n","s1_val = np.load(open(f'datasets/s1_val.npy', 'rb'))\n","s2_val = np.load(open(f'datasets/s2_val.npy', 'rb'))\n","s1_test = np.load(open(f'datasets/s1_test.npy', 'rb'))\n","s2_test = np.load(open(f'datasets/s2_test.npy', 'rb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from src.preprocessing import correct_baseline\n","\n","def preprocess_dataset(dataset):\n","    dataset = np.apply_along_axis(lambda x: correct_baseline(x, ww_min=100), 1, dataset)\n","    pipeline = StandardScaler().fit(dataset)\n","    dataset = pipeline.transform(dataset)\n","    return dataset, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s1_train, s1_train_pipeline = preprocess_dataset(s1_train)\n","s2_train, s2_train_pipeline = preprocess_dataset(s2_train)\n","s1_val,   s1_val_pipeline   = preprocess_dataset(s1_val)\n","s2_val,   s2_val_pipeline   = preprocess_dataset(s2_val)\n","s1_test,  s1_test_pipeline  = preprocess_dataset(s1_test)\n","s2_test,  s2_test_pipeline  = preprocess_dataset(s2_test)"]},{"cell_type":"markdown","metadata":{},"source":["# data exploration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.visualization import intensity_map, plot_spectra\n","from src.preprocessing import match_wavelengths\n","from plotly.express import colors\n","\n","def hyperspectral_image(dataset, pipeline, dim, wavelength_from=None, wavelength_to=None):\n","    fig = intensity_map(pipeline.inverse_transform(dataset), dim=dim, index_type=INDEX, start=wavelength_from, end=wavelength_to)\n","    fig.update_layout(yaxis=dict(scaleanchor='x'))\n","    return fig\n","\n","\n","def s1_vs_s2_spectra(s1_spectrum, s2_spectrum, s1_pipeline, s2_pipeline, **kwargs):\n","    s1_spectrum, s2_spectrum, calibration = match_wavelengths(\n","        s1_pipeline.inverse_transform([s1_spectrum]),\n","        s2_pipeline.inverse_transform([s2_spectrum]),\n","        s1_wavelengths,\n","        s2_wavelengths,\n","    )\n","    fig = plot_spectra(\n","        np.vstack((s1_spectrum, s2_spectrum)),\n","        calibration=calibration,\n","        labels=['s1 spectrum', 's2 spectrum'],\n","        **kwargs\n","    )\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperspectral_image(s1_test, s1_test_pipeline, TEST_DIM).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["id = 5\n","\n","s1_vs_s2_spectra(\n","    s1_test[id],\n","    s2_test[id],\n","    s1_test_pipeline,\n","    s2_test_pipeline,\n","    title=f'spectra at id = {id} of MASTER test (s1 spectrum) and SUBORDINATE test (s2 spectrum) datasets'\n",").show()\n","\n","del id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s1_vs_s2_spectra(\n","    s1_test.mean(axis=0),\n","    s2_test.mean(axis=0),\n","    s1_test_pipeline,\n","    s2_test_pipeline,\n","    title = 'mean spectra from the MASTER train and MASTER test datasets after preprocessing'    \n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_spectra(\n","    np.vstack((\n","        s1_train_pipeline.inverse_transform([s1_train.mean(axis=0)]),\n","        s1_test_pipeline.inverse_transform([s1_test.mean(axis=0)]),\n","    )),\n","    calibration=s1_wavelengths,\n","    labels=['train spectrum', 'test_spectrum'],\n","    colormap=colors.qualitative.Set1,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# model definition and training"]},{"cell_type":"markdown","metadata":{},"source":["## knn baseline (knn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ih-Na7v0RgaS"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","neighbors = [5, 7, 10, 12, 15]\n","cosines = []\n","mses = []\n","rses = []\n","\n","knn = None\n","best_k = -1\n","best_score = -1\n","\n","# optimize k\n","for n_neighbors in tqdm(neighbors):\n","    new = KNeighborsRegressor(n_neighbors=n_neighbors).fit(s2_train, s1_train)\n","    predicted = new.predict(s2_val)\n","    cosines.append(rowwise_cosine(s1_val, predicted).mean())\n","    mses.append(rowwise_euclid(s1_val, predicted).mean())\n","    rses.append(rse(s1_val, predicted))\n","    if mses[-1] > best_score:\n","        best_k = n_neighbors\n","        knn = new\n","    del new, predicted\n","\n","print(\n","f'K was optimized to minimize the average euclidean distance on the validation dataset.\\n\\\n","Best k: {best_k}.\\n\\n\\\n","    All scores:\\n\\\n","    -------------------------------------------'\n",")\n","\n","for k, c, e, r in zip(neighbors, cosines, mses, rses):\n","    print(f'k: {k}, euclid: {e}, cosine: {c}, rse: {r}')"]},{"cell_type":"markdown","metadata":{},"source":["## mean baseline (dummy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.dummy import DummyRegressor\n","\n","dummy = DummyRegressor(strategy='mean').fit(s2_train, s1_train)"]},{"cell_type":"markdown","metadata":{},"source":["## proposed model (model)"]},{"cell_type":"markdown","metadata":{},"source":["### autoencoder (vae)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.keras_mixins import AnnealingCallback, Sampling, VariationalAutoencoder\n","\n","def build_model(hp, regularizer=tf.keras.regularizers.l2(l2=1e-6)):\n","    neuron_counts = [\n","                    hp.Choice('first_layer', values=[2048, 1024, 128]),\n","                    hp.Choice('second_layer', values=[512, 64, 32]),\n","                    hp.Choice('bottleneck', values=[8, 32, 64]),\n","    ]\n","    neuron_counts.append(neuron_counts[1])\n","    neuron_counts.append(neuron_counts[0])\n","    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n","    bottleneck = neuron_counts.index(min(neuron_counts))\n","    bottleneck_size = neuron_counts[bottleneck]\n","\n","    # encoder\n","    inputs = tf.keras.Input(shape=s1_wavelengths.shape)\n","    x = tf.keras.layers.Dense(neuron_counts[0], activation='leaky_relu', kernel_regularizer=regularizer)(inputs)\n","    for nodes in neuron_counts[1:bottleneck]:\n","        x = tf.keras.layers.Dense(nodes, activation='leaky_relu', kernel_regularizer=regularizer)(x)\n","    z_mean = tf.keras.layers.Dense(bottleneck_size)(x)\n","    z_log_sigma = tf.keras.layers.Dense(bottleneck_size)(x)\n","    z = Sampling()([z_mean, z_log_sigma])\n","    encoder = tf.keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n","    encoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","\n","    # decoder\n","    latent_inputs = tf.keras.Input(shape=(bottleneck_size,), name='z_sampling')\n","    x = tf.keras.layers.Dense(neuron_counts[bottleneck + 1], activation='leaky_relu', kernel_regularizer=regularizer)(latent_inputs)\n","    for nodes in neuron_counts[bottleneck + 2:]:\n","        x = tf.keras.layers.Dense(nodes, activation='leaky_relu', kernel_regularizer=regularizer)(x)\n","    outputs = tf.keras.layers.Dense(s1_wavelengths.shape[0])(x)\n","    decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')\n","    decoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","\n","    outputs = decoder(encoder(inputs)[2])\n","    vae = tf.keras.Model(inputs, outputs, name='vae_mlp')\n","\n","    vae = VariationalAutoencoder(encoder, decoder)\n","    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","\n","    return vae"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras_tuner\n","\n","epochs = 50\n","batch_size = 128\n","\n","tuner = keras_tuner.Hyperband(build_model,\n","                    objective='val_loss',\n","                    max_epochs=50,\n","                    #overwrite = True,\n","                    )\n","tuner.search(\n","    s1_train, epochs=epochs, batch_size=batch_size, validation_data=(s1_val,),\n","    verbose=2, callbacks=[\n","        tf.keras.callbacks.EarlyStopping(monitor='val_reconstruction_loss', patience=5, min_delta=1e-4,),\n","        AnnealingCallback(epochs),\n","    ]\n",")\n","\n","vae = tuner.get_best_models()[0]\n","# restore weights\n","vae.predict(s1_train[:5])\n","\n","del epochs, batch_size, tuner"]},{"cell_type":"markdown","metadata":{},"source":["### mlp (insert)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bottleneck_size = vae.get_layer('encoder').layers[-1].output_shape[-1]\n","\n","def build_model(hp, regularizer=tf.keras.regularizers.l2(l2=1e-6)):\n","    neuron_counts = [\n","                    hp.Choice('first_layer', values=[2048, 1024, 512, 128]),\n","                    hp.Choice('second_layer', values=[1024, 512, 128]),\n","    ]\n","    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n","    input = tf.keras.layers.Input(shape=s2_wavelengths.shape, name='insert_in')\n","    x = tf.keras.layers.Dense(neuron_counts[0], kernel_regularizer=regularizer, activation='leaky_relu')(input)\n","    for n in neuron_counts[1:]:\n","        x = tf.keras.layers.Dense(n, kernel_regularizer=regularizer, activation='leaky_relu')(x)\n","    x = tf.keras.layers.Dense(bottleneck_size, name='output')(x)\n","\n","    insert = tf.keras.Model(input, x, name='Inserter')\n","    insert.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n","\n","    return insert"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 128\n","epochs = 100\n","\n","train_latent = vae.encode(s1_train)\n","val_latent = vae.encode(s1_val)\n","tuner = keras_tuner.Hyperband(build_model,\n","                    objective='val_loss',\n","                    max_epochs=50,\n","                    overwrite = True,\n","                    )\n","tuner.search(\n","    s2_train, train_latent, validation_data=(s2_val, val_latent), epochs=epochs,\n","    batch_size=batch_size, verbose=2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-5,)]\n",")\n","del train_latent, val_latent\n","\n","insert = tuner.get_best_models()[0]\n","#restore weights\n","insert.predict(s2_train[:5])\n","\n","del epochs, batch_size, tuner"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(s2_wavelengths.shape[0])\n","\n","s2_val.shape[1]"]},{"cell_type":"markdown","metadata":{},"source":["### combined model (model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learning_rate = 1e-6\n","metrics=[tf.keras.losses.cosine_similarity]\n","\n","decoder = vae.decoder\n","input = tf.keras.layers.Input(shape=s2_wavelengths.shape, name='model_in')\n","model_encoder = tf.keras.models.clone_model(insert)\n","model_encoder.set_weights(insert.get_weights())\n","model_decoder = tf.keras.models.clone_model(decoder)\n","model_decoder.set_weights(decoder.get_weights())\n","encoded = model_encoder(input)\n","decoded = model_decoder(encoded)\n","model = tf.keras.Model(input, decoded, name=\"Model\")\n","model_encoder.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n","model_decoder.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n","model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=metrics)\n","\n","del learning_rate, metrics, input, model_encoder, model_decoder, encoded, decoded"]},{"cell_type":"markdown","metadata":{},"source":["## mlp baseline (mlp_baseline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learning_rate = 1e-4\n","metrics=[tf.keras.losses.cosine_similarity]\n","\n","input = tf.keras.layers.Input(shape=s2_wavelengths.shape, name='baseline_in')\n","mlp_encoder = tf.keras.models.clone_model(insert)\n","mlp_decoder = tf.keras.models.clone_model(decoder)\n","encoded = model_encoder(input)\n","decoded = model_decoder(encoded)\n","mlp_baseline = tf.keras.Model(input, decoded, name=\"Model\")\n","mlp_encoder.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n","mlp_decoder.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n","mlp_baseline.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=metrics)\n","\n","del learning_rate, metrics, input, mlp_encoder, mlp_decoder, encoded, decoded"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs = 100\n","batch_size = 128\n","\n","mlp_baseline.fit(\n","    s2_train, s1_train,\n","    validation_data=(s2_val, s1_val),\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    verbose=2,\n","    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-5)]\n",")\n","\n","del epochs, batch_size"]},{"cell_type":"markdown","metadata":{},"source":["# evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# expected reduction in size of shared data\n","bottleneck = 64\n","print(1 - (bottleneck * TEST_DIM[0] * TEST_DIM[1] + model.get_layer('decoder').count_params()) / (s1_wavelengths.shape[0] * TEST_DIM[0] * TEST_DIM[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.visualization import error_map\n","from src.rowwise_metrics import rowwise_euclid, rowwise_cosine\n","from src.metrics import rse\n","\n","def evaluate_model(model, s2_dataset, s1_dataset, s1_pipeline):\n","    predicted = s1_pipeline.iverse_transform(model.predict(s2_dataset))\n","    original = s1_pipeline.inverse_transform(s1_dataset)\n","\n","    print(\n","    f\"    Numeric results\\n\\\n","        -------------------------------------------\\n\\\n","    RSE                        : {rse(original, predicted)}\\n\\\n","    average Euclidean distance : {rowwise_euclid(original, predicted).mean()}\\n\\\n","    average cosine distance    : {rowwise_cosine(original, predicted).mean()}\\n\"\n","    )\n","    return original, predicted"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dim = TEST_DIM\n","original, predicted = evaluate_model(model, s2_test, s1_test, s1_test_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["error_map(\n","    y_true=original, y_pred=predicted,\n","    dim=dim,\n","    index_type=INDEX,\n","    rowwise_error=rowwise_euclid,\n","    title='Euclidean distance map',\n","    add_stats=True,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["error_map(\n","    y_true=original, y_pred=predicted,\n","    dim=dim,\n","    index_type=INDEX,\n","    rowwise_error=rowwise_cosine,\n","    title='Cosine distance map',\n","    add_stats=True,\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from src.visualization import plot_map\n","\n","# clustering experiment\n","\n","## optimize k\n","ks = range(3, 12)\n","scores = []\n","best_score = -1\n","best_k = -1\n","kmeans = None\n","for n_clusters in tqdm(ks):\n","    new = KMeans(n_clusters=n_clusters, random_state=42).fit(original)\n","    score = silhouette_score(\n","        original, new.predict(original),\n","        random_state=42, sample_size=25000\n","    )\n","    scores.append(score)\n","    if score > best_score:\n","        best_score = score\n","        best_k = n_clusters\n","        kmeans = new\n","\n","print(\n","f'K was optimized to minimize the silhouette score on the evaluation (originally set to test) dataset.\\n\\\n","Best k: {best_k}.\\n\\n\\\n","    All scores:\\n\\\n","    -------------------------------------------'\n",")\n","for k, score in zip(ks, scores):\n","    print(f'k: {k}, silhouette score: {score}')\n","\n","\n","## k-score\n","original_labels = kmeans.predict(original)\n","predicted_labels = kmeans.predict(predicted.astype(float))\n","\n","plot_map(\n","    values=original_labels,\n","    dim=dim,\n","    index_type=INDEX,\n","    title='labels assigned to original spectra',\n",").show()\n","\n","plot_map(\n","    values = 1 - np.equal(predicted_labels, original_labels).astype(int),\n","    dim=dim,\n","    index_type=INDEX,\n","    title='predicted spectra with different label than corresponding original',\n",").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# samples\n","\n","x, y = 0, 0\n","\n","def id_from_snake_index(x, y, dim):\n","    return y * dim[1] + (x if not y % 2 else dim[1] - x - 1)\n","\n","id = id_from_snake_index(x, y, dim)\n","\n","plot_spectra(\n","    np.concatenate((original[id], predicted[id])),\n","    calibration=s1_wavelengths,\n","    title=f'Sample at x, y coordinates: ({x}, {y})',\n","    labels=['original', 'predicted']\n",").show()"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"TFL_baseline.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.9 ('.venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"06502dbc2a927cf74c4eb83114becde1fc721826ee8b0c647250777228276813"}}},"nbformat":4,"nbformat_minor":0}
